{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-02-03T15:55:44.954348Z",
     "start_time": "2025-02-03T15:55:44.627371Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from keras.src.utils import split_dataset\n",
    "from pandas.io.sas.sas_constants import column_name_text_subheader_length\n",
    "from pyarrow.dataset import dataset\n",
    "from torchgen.executorch.api.et_cpp import return_type\n",
    "\n",
    "df= messages= pd.read_csv(r'C:\\Users\\Suman Kar\\Downloads\\SMSSpamCollection (1).txt',sep='\\t',names=['label','message'])\n",
    "\n",
    "df['label']=df['label'].map({'ham':0,'spam':1})\n",
    "dataset= Dataset.from_pandas(df)\n",
    "\n",
    "print(dataset)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['label', 'message'],\n",
      "    num_rows: 5572\n",
      "})\n"
     ]
    }
   ],
   "execution_count": 76
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "TFDistilBertForSequenceClassification{FOR SENTIMENT ANALYSIS}\n",
   "id": "599797a646f30da7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-03T15:56:14.523348Z",
     "start_time": "2025-02-03T15:56:07.866484Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import TFDistilBertForSequenceClassification, AutoTokenizer\n",
    "\n",
    "# Load DistilBERT model for binary classification\n",
    "model = TFDistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=2)\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n"
   ],
   "id": "d64419b598489135",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Suman Kar\\AppData\\Roaming\\Python\\Python312\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Suman Kar\\AppData\\Roaming\\Python\\Python312\\site-packages\\tf_keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertForSequenceClassification: ['vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights or buffers of the TF 2.0 model TFDistilBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "execution_count": 78
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-03T15:57:03.605163Z",
     "start_time": "2025-02-03T15:57:02.272620Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import tensorflow as tf\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizers= AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"message\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "# Apply tokenization\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "print(tokenized_datasets)"
   ],
   "id": "44e2e3166ae6e3a0",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/5572 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "22d84e2356ed4cbba85b51fc3a1f613b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['label', 'message', 'input_ids', 'attention_mask'],\n",
      "    num_rows: 5572\n",
      "})\n"
     ]
    }
   ],
   "execution_count": 81
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-03T15:57:59.796736Z",
     "start_time": "2025-02-03T15:57:59.785878Z"
    }
   },
   "cell_type": "code",
   "source": [
    "split_dataset = dataset.train_test_split(test_size=0.2,seed=42)\n",
    "\n",
    "train_dataset=split_dataset[\"train\"]\n",
    "eval_dataset=split_dataset[\"test\"]\n",
    "\n",
    "print(train_dataset)\n",
    "print(eval_dataset)"
   ],
   "id": "6ff37e61a6cb1609",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['label', 'message'],\n",
      "    num_rows: 4457\n",
      "})\n",
      "Dataset({\n",
      "    features: ['label', 'message'],\n",
      "    num_rows: 1115\n",
      "})\n"
     ]
    }
   ],
   "execution_count": 85
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-03T15:59:40.353714Z",
     "start_time": "2025-02-03T15:59:40.236980Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "def convert_to_tf_dataset(dataset, batch_size=8):\n",
    "\n",
    "    dataset.set_format(type=\"tensorflow\",columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "\n",
    "    return dataset.to_tf_dataset(\n",
    "           columns=[\"input_ids\", \"attention_mask\"],\n",
    "           label_cols=[\"label\"],\n",
    "           shuffle=True,\n",
    "           batch_size=batch_size\n",
    "    )\n",
    "\n",
    "\n",
    "train_dataset = convert_to_tf_dataset(tokenized_datasets[\"train\"])\n",
    "eval_dataset = convert_to_tf_dataset(tokenized_datasets[\"test\"])\n"
   ],
   "id": "3ccd991d40f716d7",
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"Column train not in the dataset. Current columns in the dataset: ['label', 'message', 'input_ids', 'attention_mask']\"",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[88], line 14\u001B[0m\n\u001B[0;32m      5\u001B[0m     dataset\u001B[38;5;241m.\u001B[39mset_format(\u001B[38;5;28mtype\u001B[39m\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtensorflow\u001B[39m\u001B[38;5;124m\"\u001B[39m,columns\u001B[38;5;241m=\u001B[39m[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124minput_ids\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mattention_mask\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlabel\u001B[39m\u001B[38;5;124m\"\u001B[39m])\n\u001B[0;32m      6\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m dataset\u001B[38;5;241m.\u001B[39mto_tf_dataset(\n\u001B[0;32m      7\u001B[0m            columns\u001B[38;5;241m=\u001B[39m[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124minput_ids\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mattention_mask\u001B[39m\u001B[38;5;124m\"\u001B[39m],\n\u001B[0;32m      8\u001B[0m            label_cols\u001B[38;5;241m=\u001B[39m[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlabel\u001B[39m\u001B[38;5;124m\"\u001B[39m],\n\u001B[0;32m      9\u001B[0m            shuffle\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[0;32m     10\u001B[0m            batch_size\u001B[38;5;241m=\u001B[39mbatch_size\n\u001B[0;32m     11\u001B[0m     )\n\u001B[1;32m---> 14\u001B[0m train_dataset \u001B[38;5;241m=\u001B[39m convert_to_tf_dataset(\u001B[43mtokenized_datasets\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtrain\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m)\n\u001B[0;32m     15\u001B[0m eval_dataset \u001B[38;5;241m=\u001B[39m convert_to_tf_dataset(tokenized_datasets[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtest\u001B[39m\u001B[38;5;124m\"\u001B[39m])\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\datasets\\arrow_dataset.py:2780\u001B[0m, in \u001B[0;36mDataset.__getitem__\u001B[1;34m(self, key)\u001B[0m\n\u001B[0;32m   2778\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21m__getitem__\u001B[39m(\u001B[38;5;28mself\u001B[39m, key):  \u001B[38;5;66;03m# noqa: F811\u001B[39;00m\n\u001B[0;32m   2779\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Can be used to index columns (by string names) or rows (by integer index or iterable of indices or bools).\"\"\"\u001B[39;00m\n\u001B[1;32m-> 2780\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_getitem\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkey\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\datasets\\arrow_dataset.py:2764\u001B[0m, in \u001B[0;36mDataset._getitem\u001B[1;34m(self, key, **kwargs)\u001B[0m\n\u001B[0;32m   2762\u001B[0m format_kwargs \u001B[38;5;241m=\u001B[39m format_kwargs \u001B[38;5;28;01mif\u001B[39;00m format_kwargs \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m {}\n\u001B[0;32m   2763\u001B[0m formatter \u001B[38;5;241m=\u001B[39m get_formatter(format_type, features\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_info\u001B[38;5;241m.\u001B[39mfeatures, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mformat_kwargs)\n\u001B[1;32m-> 2764\u001B[0m pa_subtable \u001B[38;5;241m=\u001B[39m \u001B[43mquery_table\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_data\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkey\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mindices\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_indices\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   2765\u001B[0m formatted_output \u001B[38;5;241m=\u001B[39m format_table(\n\u001B[0;32m   2766\u001B[0m     pa_subtable, key, formatter\u001B[38;5;241m=\u001B[39mformatter, format_columns\u001B[38;5;241m=\u001B[39mformat_columns, output_all_columns\u001B[38;5;241m=\u001B[39moutput_all_columns\n\u001B[0;32m   2767\u001B[0m )\n\u001B[0;32m   2768\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m formatted_output\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\datasets\\formatting\\formatting.py:590\u001B[0m, in \u001B[0;36mquery_table\u001B[1;34m(table, key, indices)\u001B[0m\n\u001B[0;32m    588\u001B[0m         _raise_bad_key_type(key)\n\u001B[0;32m    589\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(key, \u001B[38;5;28mstr\u001B[39m):\n\u001B[1;32m--> 590\u001B[0m     \u001B[43m_check_valid_column_key\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkey\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtable\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcolumn_names\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    591\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    592\u001B[0m     size \u001B[38;5;241m=\u001B[39m indices\u001B[38;5;241m.\u001B[39mnum_rows \u001B[38;5;28;01mif\u001B[39;00m indices \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m table\u001B[38;5;241m.\u001B[39mnum_rows\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\datasets\\formatting\\formatting.py:527\u001B[0m, in \u001B[0;36m_check_valid_column_key\u001B[1;34m(key, columns)\u001B[0m\n\u001B[0;32m    525\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21m_check_valid_column_key\u001B[39m(key: \u001B[38;5;28mstr\u001B[39m, columns: List[\u001B[38;5;28mstr\u001B[39m]) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    526\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m key \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m columns:\n\u001B[1;32m--> 527\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mColumn \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mkey\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m not in the dataset. Current columns in the dataset: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mcolumns\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[1;31mKeyError\u001B[0m: \"Column train not in the dataset. Current columns in the dataset: ['label', 'message', 'input_ids', 'attention_mask']\""
     ]
    }
   ],
   "execution_count": 88
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
